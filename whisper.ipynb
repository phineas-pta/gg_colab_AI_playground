{
	"cells": [
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Whisper\n",
				"\n",
				"general-purpose speech recognition model"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"!nvidia-smi"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"download audio"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"!pip install -q yt-dlp"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from yt_dlp import YoutubeDL\n",
				"\n",
				"ydl_opts = {\n",
				"\t\"format\": \"bestaudio\",\n",
				"\t\"outtmpl\": \"audio.%(ext)s\",\n",
				"\t\"overwrites\": True,\n",
				"\t\"postprocessors\": [{\"key\": \"FFmpegExtractAudio\", \"preferredcodec\": \"wav\"}]\n",
				"}"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"with YoutubeDL(ydl_opts) as ydl:\n",
				"\tydl.download(\"0ncZ-4BENRU\")"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Voice extraction\n",
				"\n",
				"for better transcription\n",
				"\n",
				"using SOTA faceboook `demucs`"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%pip install -q demucs"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import torchaudio\n",
				"from demucs.pretrained import get_model\n",
				"from demucs.separate import load_track\n",
				"from demucs.apply import apply_model"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"demucs_model = get_model(\"htdemucs\").cuda()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"raw_audio = load_track(\"audio.wav\", demucs_model.audio_channels, demucs_model.samplerate)\n",
				"# should not be on GPU because sometimes not enough VRAM\n",
				"\n",
				"if raw_audio.dim() == 1:\n",
				"\traw_audio = raw_audio[None, None].repeat_interleave(2, -2)\n",
				"elif raw_audio.shape[-2] == 1:\n",
				"\traw_audio = raw_audio.repeat_interleave(2, -2)\n",
				"elif raw_audio.dim() < 3:\n",
				"\traw_audio = raw_audio[None]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"demucs_extract = apply_model(demucs_model, raw_audio, device=\"cuda\", split=True, overlap=.25)\n",
				"torchaudio.save(\"audio.vocals.wav\", demucs_extract[0, demucs_model.sources.index(\"vocals\")].mean(0)[None], demucs_model.samplerate)"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Original model"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%pip install -q openai-whisper"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"5 models:\n",
				"\n",
				"|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
				"|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
				"|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
				"|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
				"| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
				"| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
				"| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import whisper\n",
				"model = whisper.load_model(\"large\")\n",
				"writer = whisper.utils.get_writer(\"srt\", \".\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"audio = whisper.load_audio(\"audio.wav\")\n",
				"result = model.transcribe(audio, verbose=False)"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"possible extension: Silero VAD for better noise canceling"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"save as subtitle `.srt` file"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"writer(result, \"yolo.srt\")"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### load fine-tuned model from `huggingface`\n",
				"\n",
				"should have same size as `openai/whisper-large-v2`"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"!wget https://huggingface.co/vumichien/whisper-large-v2-mix-jp/resolve/main/pytorch_model.bin"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import torch\n",
				"hf_state_dict = torch.load(\"pytorch_model.bin\", map_location=torch.device(\"cpu\")) # in case not enough VRAM"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"compare `model.state_dict.keys()` vs `hf_state_dict.keys()`\n",
				"\n",
				"ref:\n",
				" - https://github.com/openai/whisper/discussions/830#discussioncomment-4652413\n",
				" - https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/convert_openai_to_hf.py"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"rename_keys = lambda text: (text\n",
				"\t.replace(\"model.\", \"\")\n",
				"\t.replace(\"layers\", \"blocks\")\n",
				"\t.replace(\"fc1\", \"mlp.0\")\n",
				"\t.replace(\"fc2\", \"mlp.2\")\n",
				"\t.replace(\"final_layer_norm\", \"mlp_ln\")\n",
				"\t.replace(\".self_attn.q_proj\", \".attn.query\")\n",
				"\t.replace(\".self_attn.k_proj\", \".attn.key\")\n",
				"\t.replace(\".self_attn.v_proj\", \".attn.value\")\n",
				"\t.replace(\".self_attn_layer_norm\", \".attn_ln\")\n",
				"\t.replace(\".self_attn.out_proj\", \".attn.out\")\n",
				"\t.replace(\".encoder_attn.q_proj\", \".cross_attn.query\")\n",
				"\t.replace(\".encoder_attn.k_proj\", \".cross_attn.key\")\n",
				"\t.replace(\".encoder_attn.v_proj\", \".cross_attn.value\")\n",
				"\t.replace(\".encoder_attn_layer_norm\", \".cross_attn_ln\")\n",
				"\t.replace(\".encoder_attn.out_proj\", \".cross_attn.out\")\n",
				"\t.replace(\"decoder.layer_norm.\", \"decoder.ln.\")\n",
				"\t.replace(\"encoder.layer_norm.\", \"encoder.ln_post.\")\n",
				"\t.replace(\"embed_tokens\", \"token_embedding\")\n",
				"\t.replace(\"encoder.embed_positions.weight\", \"encoder.positional_embedding\")\n",
				"\t.replace(\"decoder.embed_positions.weight\", \"decoder.positional_embedding\")\n",
				"\t.replace(\"layer_norm\", \"ln_post\")\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"for key in [*hf_state_dict]:\n",
				"\tnew_key = rename_keys(key)\n",
				"\thf_state_dict[new_key] = hf_state_dict.pop(key)\n",
				"model.load_state_dict(hf_state_dict)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"result = model.transcribe(audio, verbose=False, language=\"ja\", task=\"translate\")"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Faster Whisper\n",
				"\n",
				"able to run `large` model as `float16` with <6GB VRAM"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%pip install -q faster-whisper transformers"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import faster_whisper\n",
				"from tqdm import tqdm\n",
				"\n",
				"model = faster_whisper.WhisperModel(\"large-v2\", device=\"cuda\") # compute_type=\"float16\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def convert_to_hms(seconds: float) -> str:\n",
				"\thours, remainder = divmod(seconds, 3600)\n",
				"\tminutes, seconds = divmod(remainder, 60)\n",
				"\tmilliseconds = round((seconds % 1) * 1000)\n",
				"\toutput = f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{milliseconds:03}\"\n",
				"\treturn output\n",
				"\n",
				"def convert_seg(segment: faster_whisper.transcribe.Segment) -> str:\n",
				"\treturn f\"{convert_to_hms(segment.start)} --> {convert_to_hms(segment.end)}\\n{segment.text.strip()}\""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"segments, info = model.transcribe(\"audio.wav\", vad_filter=True) # Silero VAD model to remove silence\n",
				"\n",
				"full_txt = []\n",
				"timestamps = 0.0  # for progress bar\n",
				"with tqdm(total=round(info.duration, 2), unit=\" audio seconds\") as pbar:\n",
				"\tfor i, segment in enumerate(segments, start=1):\n",
				"\t\tfull_txt.append(f\"{i}\\n{convert_seg(segment)}\\n\\n\")\n",
				"\t\tpbar.update(segment.end - timestamps)\n",
				"\t\ttimestamps = segment.end\n",
				"\tif timestamps < info.duration:\n",
				"\t\tpbar.update(info.duration - timestamps)\n",
				"\n",
				"with open(\"yolo.srt\", mode=\"w\", encoding=\"UTF-8\") as f:\n",
				"\tf.writelines(full_txt)"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### load fine-tuned model from `huggingface`\n",
				"\n",
				"should have same size as `openai/whisper-large-v2`"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"!ct2-transformers-converter --model vumichien/whisper-large-v2-mix-jp --output_dir test-ct2 --quantization float16 # in case not enough RAM"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"model = faster_whisper.WhisperModel(\"test-ct2\", device=\"cuda\", compute_type=\"float16\")"
			]
		}
	],
	"metadata": {
		"accelerator": "GPU",
		"colab": {
			"collapsed_sections": [],
			"name": "whisper.ipynb",
			"private_outputs": true,
			"provenance": []
		},
		"gpuClass": "standard",
		"kernelspec": {
			"display_name": "Python 3",
			"name": "python3"
		},
		"language_info": {
			"name": "python"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
