{
	"cells": [
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Text generation with LLM"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"!nvidia-smi"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"!python -m torch.utils.collect_env"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Medium-sized"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%pip install -q accelerate bitsandbytes transformers"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from transformers import AutoModelForCausalLM, AutoTokenizer\n",
				"# import torch"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"MODEL_NAME = \"EleutherAI/gpt-j-6B\"\n",
				"model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_8bit=True, device_map=\"auto\")\n",
				"# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16).cuda() # revision=\"float16\", low_cpu_mem_usage=True\n",
				"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"inputs = tokenizer(\"The Belgian national football team \", return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
				"outputs = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200)\n",
				"print(tokenizer.decode(outputs[0]))"
			]
		},
		{
			"attachments": {},
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Full-sized with PETALS"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%pip install -q petals"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"from transformers import BloomTokenizerFast \n",
				"from petals import DistributedBloomForCausalLM\n",
				"\n",
				"MODEL_NAME = \"bigscience/bloom-petals\" # `bloomz` for more chatbot behaviour\n",
				"tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)\n",
				"model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME).cuda()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"inputs = tokenizer('A cat in French is \"', return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
				"outputs = model.generate(inputs, max_new_tokens=3)\n",
				"print(tokenizer.decode(outputs[0]))"
			]
		}
	],
	"metadata": {
		"accelerator": "GPU",
		"colab": {
			"collapsed_sections": [],
			"name": "LLM.ipynb",
			"private_outputs": true,
			"provenance": []
		},
		"gpuClass": "standard",
		"kernelspec": {
			"display_name": "Python 3",
			"name": "python3"
		},
		"language_info": {
			"name": "python"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}
